{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".csv with all manual labels applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file\n",
    "reddit = joblib.load('./data/pickle/reddit_labelled-sample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "reddit[\"processed_text\"] = reddit[\"text\"].apply(f.token_and_lemmatize_rob)\n",
    "\n",
    "# Features and labels\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(reddit[\"processed_text\"])  # Tfidf vectorization\n",
    "y = reddit[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=13,\n",
    "    stratify=y\n",
    ")\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test RoBERTa (Robustly Optimized BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense and prepare datasets\n",
    "X_train_text = [\" \".join(map(str, row)) for row in X_train.toarray()]\n",
    "X_test_text = [\" \".join(map(str, row)) for row in X_test.toarray()]\n",
    "\n",
    "train_data = {\"text\": X_train_text, \"labels\": y_train.tolist()}\n",
    "test_data = {\"text\": X_test_text, \"labels\": y_test.tolist()}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize using Hugging Face tokenizer\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742eeebf15bb41c7917394f695841046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1056 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a744a3f9e5a4759bc388e4ded5de373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(f.tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(f.tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and train the model\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=3\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Decode predictions and calculate accuracy\n",
    "decoded_preds = encoder.inverse_transform(preds)\n",
    "decoded_labels = encoder.inverse_transform(y_test)\n",
    "\n",
    "accuracy = accuracy_score(decoded_labels, decoded_preds)\n",
    "print(f\"RoBERTa model accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        decoded_labels, decoded_preds, target_names=encoder.classes_, zero_division=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/seshat/Documents/GitHub/labor_sentiment_analysis/models/roberta_label_encoder.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the save path\n",
    "save_path = \"./models\"\n",
    "\n",
    "# Save the trained model\n",
    "roberta_model.save_pretrained(save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Save the LabelEncoder as a pickle object\n",
    "joblib.dump(\n",
    "    encoder,\n",
    "    f\"{save_path}/roberta_label_encoder.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "nb_classifier = joblib.load(\n",
    "    \"./models/custom_nb_model.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.43018867924528303\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        31\n",
      "     neutral       0.47      0.67      0.55       134\n",
      "    positive       0.33      0.24      0.28       100\n",
      "\n",
      "    accuracy                           0.43       265\n",
      "   macro avg       0.27      0.30      0.28       265\n",
      "weighted avg       0.36      0.43      0.39       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare to SAME, UNALTERED custom Naive Bayes model from above applied to this test data subset.\n",
    "# Extra testing!\n",
    "# predict\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# decode\n",
    "custom_nb_pred = encoder.inverse_transform(y_pred)\n",
    "\n",
    "y_test_decoded = encoder.inverse_transform(y_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test_decoded, custom_nb_pred)\n",
    "print(f\"Test set accuracy: {accuracy}\")\n",
    "\n",
    "# Detailed performance metrics\n",
    "print(\"Classification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test_decoded, custom_nb_pred, \n",
    "        target_names=encoder.classes_, \n",
    "        zero_division=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare, contrast and discuss these results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
