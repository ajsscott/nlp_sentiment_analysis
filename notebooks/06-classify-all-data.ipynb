{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tokenizers import Tokenizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full processed dataset\n",
    "pickle_folder = \"./data/pickle\"\n",
    "pickle_path = os.path.join(pickle_folder, \"reddit.pkl\")\n",
    "reddit = joblib.load(pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "stop_words = set(STOP_WORDS)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function using Hugging Face tokenizer\n",
    "def token_and_lemmatize_nb(text):\n",
    "        # Tokenize text using Hugging Face tokenizer\n",
    "    output = tokenizer.encode(text.lower())\n",
    "    tokens = output.tokens  # Get tokenized words\n",
    "\n",
    "    # Remove stopwords and lemmatize tokens\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Rejoin tokens into a processed string\n",
    "    return \" \".join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target\n",
    "data['text_processed'] = data[\"text\"].apply(token_and_lemmatize_nb)\n",
    "\n",
    "X = data[\"text_processed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "nb_classifier = joblib.load(\n",
    "    \"./models/custom_nb_model.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the classifier to the 'text' column to classify the text\n",
    "data['label_nb'] = nb_classifier.predict(X)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load(\n",
    "    \"./models/nb_label_encoder.pkl\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "data[\"label_nb\"] = encoder.inverse_transform(data[\"label_nb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./data/master/reddit_labelled_nb.csv\", index=False)\n",
    "pickle_path = os.path.join(pickle_folder, \"reddit_labelled_nb.pkl\")\n",
    "joblib.dump(data, pickle_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LabelEncoder\n",
    "encoder.classes_ = np.load(\n",
    "    \"./models/roberta_label_encoder.pkl\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "\n",
    "# Tokenize and predict function\n",
    "def predict_with_roberta(text):\n",
    "    # Tokenize the input text\n",
    "    save_path = \"./models\"\n",
    "    roBERTa_model = RobertaForSequenceClassification.from_pretrained(save_path)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(save_path)\n",
    "\n",
    "    roBERTa_model.eval()\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = roBERTa_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = torch.argmax(logits, axis=1).item()  # Get predicted label\n",
    "\n",
    "    # Decode the numerical prediction to original label\n",
    "    decoded_label = encoder.inverse_transform([predicted_label])[0]\n",
    "    return decoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the prediction function to the processed text column\n",
    "data[\"label_rob\"] = data[\"text\"].apply(f.predict_with_roberta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save again\n",
    "data.to_csv(\n",
    "    \"./data/master/reddit_labelled.csv\",\n",
    "    index=False,\n",
    ")\n",
    "pickle_path = os.path.join(pickle_folder, \"reddit_labelled.pkl\")\n",
    "joblib.dump(data, pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
